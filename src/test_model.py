# -*- coding: utf -8 -*-
   
import tensorflow as tf
from word2vec import comment_embedding, WordModel
import gensim.models.keyedvectors as word2vec
import numpy as np
from utils import pre_process
from underthesea import word_tokenize

text = ["San pham dep, minh rat thich chat vai nay!!!! ğŸ‘ğŸ‘ğŸ‘", "LÃ m Äƒn nhÆ° ccccc ğŸ˜¡ğŸ˜¡ğŸ˜¡", "<3", "ğŸ’¥ğŸ’¥ğŸ’¥Giao hÃ ng nhanh.ğŸ‘ ğŸ’¥ğŸ’¥ğŸ’¥", "ğŸ”¥ <3", "Shop chÃ¡n vkl", "Nchung lÃ  á»•n, giao hÃ ng nhanh kb cho â­â­â­â­â­"]
text = pre_process(text)
print(text)
for i in range(len(text)):
    text[i] = word_tokenize(text[i])

print(text)
